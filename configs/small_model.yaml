# MoR Small Language Model Configuration
model:
  vocab_size: 50257
  hidden_size: 512
  intermediate_size: 2048
  num_attention_heads: 8
  num_key_value_heads: 8
  max_position_embeddings: 2048
  rms_norm_eps: 0.000001
  rope_theta: 10000.0
  
  # MoR-specific parameters
  num_recursion_blocks: 4  # Nr in paper
  parameter_sharing_strategy: "middle_cycle"  # Options: cycle, sequence, middle_cycle, middle_sequence
  max_recursion_depth: 4
  min_recursion_depth: 1
  
  # Routing configuration
  routing_strategy: "expert_choice"  # Options: expert_choice, token_choice
  router_type: "linear"  # Linear router as specified in paper
  beta_percentile: 0.8  # Î²-percentile threshold
  auxiliary_loss_weight: 0.01
  
  # KV Cache optimization
  enable_kv_cache: true
  cache_strategy: "efficient"
  
training:
  batch_size: 8
  gradient_accumulation_steps: 4
  learning_rate: 0.0003
  weight_decay: 0.1
  warmup_steps: 1000
  max_steps: 10000
  save_steps: 1000
  eval_steps: 500
  
  # FSDP configuration for distributed training
  use_fsdp: false  # Set to true for multi-GPU
  fsdp_sharding_strategy: "full_shard"
  
data:
  dataset_name: "roneneldan/TinyStories"
  dataset_config: null
  max_length: 256
  preprocessing_num_workers: 2

# For training on wikitext dataset
# data:
#   dataset_name: "wikitext"
#   dataset_config: "wikitext-2-raw-v1"  # Small version
#   # dataset_config: "wikitext-103-raw-v1"  # Larger version
#   max_length: 512

# For training on openwebtext dataset
# data:
#   dataset_name: "openwebtext"
#   dataset_config: null
#   max_length: 512

inference:
  max_new_tokens: 100
  temperature: 0.7
  top_p: 0.9
  do_sample: true
